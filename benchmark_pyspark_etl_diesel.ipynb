{"cells":[{"cell_type":"markdown","source":["# Experimento de performance e Avaliação de Consumo do Fabric com o Pyspark\n","\n","Com este experimento simples, estou usando dados de diesel em dois cenários:\n","- Série histórica de preços de 4 anos totalizando mais de 700mb em csv;\n","- Arquivo simples de menos de 1mb com informações geofísicas;\n","\n","A transformação significava trazer de csv consolidado para delta (pyspark) e fazer umas transformações simples.\n","\n","Configuração Starter pool Oitimizado para memória tamanho médio 10 nodes. Spark 3.5"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c20504b-f44d-4120-bfc5-3e7e5f2533ed"},{"cell_type":"code","source":["# Código PySpark com o cenário da série histórica de 700mb\n","\n","import time\n","import psutil\n","from contextlib import contextmanager\n","from pyspark.sql.functions import regexp_replace, col\n","\n","class DetailedMetrics:\n","   def __init__(self):\n","       self.measures = {}\n","       \n","   @contextmanager\n","   def measure_step(self, step_name):\n","       start = time.time()\n","       start_mem = psutil.Process().memory_info().rss\n","       start_io = psutil.disk_io_counters()\n","       \n","       try:\n","           yield\n","       finally:\n","           end = time.time()\n","           end_mem = psutil.Process().memory_info().rss\n","           end_io = psutil.disk_io_counters()\n","           \n","           self.measures[step_name] = {\n","               'time': end - start,\n","               'memory': (end_mem - start_mem) / 1024 / 1024,\n","               'io_read': end_io.read_bytes - start_io.read_bytes,\n","               'io_write': end_io.write_bytes - start_io.write_bytes\n","           }\n","           print(f\"\\nMétricas para {step_name}:\")\n","           print(f\"Tempo: {self.measures[step_name]['time']:.2f}s\")\n","           print(f\"Memória: {self.measures[step_name]['memory']:.2f} MB\")\n","           print(f\"IO Leitura: {self.measures[step_name]['io_read'] / 1024 / 1024:.2f} MB\")\n","           print(f\"IO Escrita: {self.measures[step_name]['io_write'] / 1024 / 1024:.2f} MB\")\n","\n","metrics = DetailedMetrics()\n","\n","# Leitura\n","with metrics.measure_step(\"PySpark - Leitura\"):\n","   df_2021_01 = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Files/diesel/serie_historica_csv/historico_diesel_2021_01.csv\")\n","   df_2022_01 = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Files/diesel/serie_historica_csv/historico_diesel_2022_01.csv\")\n","   df_2022_02 = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Files/diesel/serie_historica_csv/historico_diesel_2022_02.csv\")\n","   df_2023_01 = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Files/diesel/serie_historica_csv/historico_diesel_2023_01.csv\")\n","   df_2023_02 = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Files/diesel/serie_historica_csv/historico_diesel_2023_02.csv\")\n","   df_2024_01 = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Files/diesel/serie_historica_csv/historico_diesel_2024_01.csv\")\n","   df_2024_02 = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Files/diesel/serie_historica_csv/historico_diesel_2024_02.csv\")\n","   df_concatenado = df_2021_01.union(df_2022_02).union(df_2022_01).union(df_2023_01).union(df_2023_02).union(df_2024_01).union(df_2024_02)\n","\n","# Transformação\n","with metrics.measure_step(\"PySpark - Transformação\"):\n","   df_processado = df_concatenado.dropDuplicates() \\\n","       .fillna({\"Valor de Compra\": \"0\", \"Complemento\": \"SEM COMPLEMENTO\"}) \\\n","       .withColumn(\"Valor de Venda\", regexp_replace(col(\"Valor de Venda\"), \",\", \".\").cast(\"double\")) \\\n","       .withColumn(\"Valor de Compra\", regexp_replace(col(\"Valor de Compra\"), \",\", \".\").cast(\"double\")) \\\n","       .withColumn(\"Margem\", col(\"Valor de Venda\") - col(\"Valor de Compra\")) \\\n","       .withColumnRenamed(\"Regiao - Sigla\", \"Regiao\") \\\n","       .withColumnRenamed(\"Estado - Sigla\", \"Estado\")\n","\n","# Escrita\n","with metrics.measure_step(\"PySpark - Escrita\"):\n","   df_processado.write.mode(\"overwrite\").parquet('Files/diesel/serie_historica_parquet/diesel_4anos_pyspark')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"c8da7e14-c036-44b5-9472-52716eccb7c8","normalized_state":"finished","queued_time":"2025-01-23T16:42:01.6055501Z","session_start_time":null,"execution_start_time":"2025-01-23T16:42:01.7426175Z","execution_finish_time":"2025-01-23T16:42:22.9209572Z","parent_msg_id":"64b7cdd3-bcb6-4b08-a192-44df09e79400"},"text/plain":"StatementMeta(, c8da7e14-c036-44b5-9472-52716eccb7c8, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nMétricas para PySpark - Leitura:\nTempo: 1.82s\nMemória: 0.00 MB\nIO Leitura: 0.45 MB\nIO Escrita: 1.66 MB\n\nMétricas para PySpark - Transformação:\nTempo: 0.06s\nMemória: 0.00 MB\nIO Leitura: 0.02 MB\nIO Escrita: 0.02 MB\n\nMétricas para PySpark - Escrita:\nTempo: 17.03s\nMemória: 0.00 MB\nIO Leitura: 12.15 MB\nIO Escrita: 21.50 MB\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ce828aa1-bace-461c-9193-5f4b23b6fc6c"},{"cell_type":"code","source":["# Código PySpark com o cenário da série pequena menor que 1mb\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","import time\n","import psutil\n","import os\n","\n","start_time = time.time()\n","initial_memory = get_memory_usage()\n","\n","# Ler arquivo sem cabeçalho\n","df_raw = spark.read.option(\"delimiter\", \";\").option(\"header\", False).csv('Files/diesel/pequenos/tabela-programas-geofisicos.csv')\n","\n","# Pegar a segunda linha para usar como header\n","header = df_raw.collect()[1]\n","\n","# Criar novo DataFrame pulando as duas primeiras linhas\n","df = spark.read.option(\"delimiter\", \";\").option(\"header\", False).csv('Files/diesel/pequenos/tabela-programas-geofisicos.csv').filter(f\"_c0 != '{header[0]}'\")\n","\n","# Renomear colunas\n","for idx, name in enumerate(header):\n","   if name is not None:\n","       df = df.withColumnRenamed(f\"_c{idx}\", name)\n","\n","# Transformações\n","df = df.select(\n","   col(\"Nome\").alias(\"id_programa\"),\n","   col(\"Categoria\"),\n","   when(col(\"Natureza\") == 'Não-Exclusivo', 'NAO_EXCLUSIVO').otherwise('EXCLUSIVO').alias(\"tipo_natureza\"),\n","   to_date(col(\"Inicio\")).alias(\"data_inicio\"),\n","   to_date(col(\"Término Real\")).alias(\"data_termino\"),\n","   col(\"Tecnologia\"),\n","   col(\"Bacia\"),\n","   coalesce(col(\"Operadora\"), lit(\"NAO_INFORMADO\")).alias(\"operadora\"),\n","   months_between(to_date(col(\"Término Real\")), to_date(col(\"Inicio\"))).alias(\"duracao_meses\"),\n","   when(col(\"Bacia\") == \"Santos\", \"BACIA_SANTOS\").when(col(\"Bacia\") == \"Campos\", \"BACIA_CAMPOS\").otherwise(\"OUTRAS\").alias(\"categoria_bacia\"),\n","   when(col(\"Tecnologia\") == \"Sísmica 3D\", \"SISMICA_3D\")\n","   .when(col(\"Tecnologia\") == \"Magnetometria\", \"MAGNETOMETRIA\")\n","   .when(col(\"Tecnologia\") == \"Ocean Bottom Nodes\", \"OBN\")\n","   .otherwise(\"OUTROS\").alias(\"tipo_tecnologia\")\n",")\n","\n","query_time = time.time() - start_time\n","\n","df.write.format(\"delta\").mode(\"overwrite\").save(\"Files/diesel/pequenos/programas_geofisicos_benchmark_spark\")\n","\n","total_time = time.time() - start_time\n","peak_memory = get_memory_usage() - initial_memory\n","\n","print(f\"Métricas de Performance:\")\n","print(f\"Tempo total: {total_time:.2f} segundos\")\n","print(f\"Tempo query: {query_time:.2f} segundos\")\n","print(f\"Memória utilizada: {peak_memory:.2f} MB\")\n","print(f\"Linhas processadas: {df.count()}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"c8da7e14-c036-44b5-9472-52716eccb7c8","normalized_state":"finished","queued_time":"2025-01-23T16:36:45.6982326Z","session_start_time":null,"execution_start_time":"2025-01-23T16:36:45.8627422Z","execution_finish_time":"2025-01-23T16:36:54.1825644Z","parent_msg_id":"fe97d7c3-ec55-4e1b-9402-bd903e5efbcd"},"text/plain":"StatementMeta(, c8da7e14-c036-44b5-9472-52716eccb7c8, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Métricas de Performance:\nTempo total: 6.55 segundos\nTempo query: 0.82 segundos\nMemória utilizada: 3.61 MB\nLinhas processadas: 3602\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"38273c7f-efea-49e5-9519-19fdf14cb13c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"3ddfcc4e-67b8-4986-8456-bed0cd76227d","known_lakehouses":[{"id":"3ddfcc4e-67b8-4986-8456-bed0cd76227d"}],"default_lakehouse_name":"lh_demo","default_lakehouse_workspace_id":"68bcd327-3bbc-4021-b107-835abd6aef0c"}}},"nbformat":4,"nbformat_minor":5}